<!DOCTYPE HTML>
<html lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <title>Antti Hietanen</title>
        <meta name="author" content="Antti Hietanen">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <link rel="stylesheet" type="text/css" href="assets/stylesheet.css">
    </head>


    <!-- bio -->
    <body>
    <table style="width:100%;max-width:800px;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tr style="padding:0px">
            <td style="padding:0px">
                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                    <tr style="padding:0px">
                    <td style="padding:2.5%;width:63%;vertical-align:middle">
                    <p style="text-align:center">
                        <name>Antti Hietanen</name>
                    </p>

                    <p>
                        I am currently working as a senior autonomous vehicle engineer at <a href="https://sensible4.fi/">Sensible4</a>, making self-driving technology for all weather conditions.
                    </p>
                    <p>
                        I completed my PhD at Tampere University (previously known as Tampere University of Technology), where I was advised by <a href="https://webpages.tuni.fi/vision/public_pages/JoniKamarainen/index.html">Joni-Kristian Kämäräinen</a> and <a href="https://fi.linkedin.com/in/minna-lanz-70b5553">Minna Lanz.</a>
                        During my studies, I was a member of the <a href="https://research.tuni.fi/vision/">Computer Vision Group</a> and the <a href="https://research.tuni.fi/productionsystems/">Intelligent Production Systems Group.</a>
                    </p>
                        My research interests are computer vision and machine learning and their applications in robotics, such as vision-based robot guidance and human-robot collaboration.
                        In general, I like to see piece of heavy metal moving based on your implemented algorithm.
                    </p>
                    </td>
                        <td style="padding:2.5%;width:40%;max-width:40%">
                            <a href="images/face.jpeg"><img style="width:120%;max-width:120%" alt="profile photo" src="images/face.jpg" class="hoverZoomLink"></a>
                        </td>
                    </tr>
                </table>


                <!-- Research Project -->
                <heading>Research Projects</heading>


                <!-- Paper 6 -->
                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/succ_prob3.png" alt="blind-date" width="190">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://www.sciencedirect.com/science/article/pii/S0921889021000956">
                            <papertitle>Benchmarking Pose Estimation for Robot Manipulation</papertitle>
                        </a>
                        <br>
                        <strong>Antti Hietanen</strong>, Jyrki Latokartano, Alessandro Foi, Roel Pieters, Ville Kyrki, Minna Lanz and Joni-Kristian Kämäräinena
                        <br>
                        <em>Robotics and Autonomous Systems</em>, 2021
                        <br>
                        <a href="https://www.sciencedirect.com/science/article/pii/S0921889021000956">paper</a> |
                        <a href="https://www.youtube.com/watch?v=g4e_-p4fTEI">video</a>
                        <p>

                            <!--
                            The previous works on object pose estimation evaluation have mainly focused on metrics that rank the estimated poses solely based on the visual perspective i.e. how well two geometric surfaces are aligned.
                            However, it is unclear how well the existing metrics can validate the estimated pose for a real robotic task.
                            To address this, we propose a probabilistic evaluation metric that ranks an estimated object pose based on the conditional probability of completing a robotic task given this estimated pose.
                            In addition, we present a procedure to generate automatically a large number of random grasp poses and corresponding task outcomes that are then used to estimate the grasp conditional probabilities.
                            In the experiments, the metric was found to be more realistic for measuring the estimated pose “goodness” for a given manipulation task compared to prior art.
                            -->
                            We propose a probabilistic evaluation metric that ranks an estimated object pose based on the conditional probability of completing a robotic task given this estimated pose.
                            In addition, we present a procedure to generate automatically a large number of random grasp poses and corresponding task outcomes that are then used to estimate the grasp conditional probabilities.
                            In the experiments, the metric is found to be more realistic for measuring the estimated pose “goodness” for a given manipulation task compared to prior art.

                        </p>
                    </td>
                </tr>
                </tbody>
                </table>

                <!-- Paper 5 -->
                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/hololens_ui.png" alt="blind-date" width="190">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://www.sciencedirect.com/science/article/pii/S0736584519307355">
                            <papertitle>AR-based Interaction for Human-robot Collaboration</papertitle>
                        </a>
                        <br>
                        <strong>Antti Hietanen</strong>, Roel Pieters, Minna Lanz, Jyrki Latokartano and Joni-Kristian Kämäräinen
                        <br>
                        <em>Robotics and Computer-Integrated Manufacturing</em>, 2020
                        <br>
                        <a href="https://www.sciencedirect.com/science/article/pii/S0736584519307355">paper</a> |
                        <a href="https://www.youtube.com/watch?v=-WW0a-LEGLM">video</a>
                        <p>
                            <!--
                            In our latest work on human-robot collaboration domain, the usefulness and readiness level of two different AR-based devices, projector and Microsoft HoloLens, as an user-interface medium in manufacturing task, were evaluated.
                            The qualitative and quantitative results from the experiments indicate that projector-based interaction can support and increase the comfort of the human operator during the task while HoloLens was found surprisingly unpractical due to various reasons.
                            -->
                            In this work, the usefulness and readiness level of two different AR-based devices, projector and Microsoft HoloLens, as an user-interface medium in manufacturing task, is evaluated.
                            The qualitative and quantitative results from the experiments indicate that projector-based interaction can support and increase the comfort of the human operator during the task while HoloLens is found surprisingly unpractical due to various reasons.
                        </p>
                    </td>
                </tr>
                </tbody>
                </table>


                <!-- Paper 4 -->
                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/motor.png" alt="blind-date" width="190">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://ieeexplore.ieee.org/document/8956446">
                            <papertitle>Proof of concept of a projection-based safety system for human-robot collaborative engine assembly</papertitle>
                        </a>
                        <br>
                        <strong>Antti Hietanen</strong>, Alireza Changizi, Minna Lanz, Joni-Kristian Kämäräinen, Pallab Ganguly, Roel Pieters and Jyrki Latokartano
                        <br>
                        <em>International Conference on on Robot & Human Interactive Communication (RO-MAN)</em>, 2019
                        <br>
                        <a href="https://ieeexplore.ieee.org/document/8956446">paper</a>
                        <p>
                            In this work, we introduce a realistic human-robot collaboration task where the co-workers conduct a diesel engine task and a work allocation schedule between human and robot resources is defined.
                            In addition, the work presents an important extension to our safety model by extending the monitored zones around the carried object since the assembly task includes heavy and sharp objects.
                        </p>
                    </td>
                </tr>
                </tbody>
                </table>


                <!-- Paper 3 -->
                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/safety.png" alt="blind-date" width="190">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://webpages.tuni.fi/vision/public_data/publications/iros2018ws.pdf">
                            <papertitle>Depth-sensor–projector safety model for human-robot collaboration</papertitle>
                        </a>
                        <br>
                        <strong>Antti Hietanen</strong>, Jussi Halme, Jyrki Latokartano, Roel Pieters, Minna Lanz and Joni-Kristian Kämäräinen
                        <br>
                        <em>International Conference on Intelligent Robots and Systems (IROS) Workshops</em>, 2018
                        <br>
                        <a href="https://webpages.tuni.fi/vision/public_data/publications/iros2018ws.pdf">paper</a> |
                        <a href="https://www.youtube.com/watch?v=CFKKANvWc3A">video</a>
                        <p>
                            A safety model for human-robot collaboration is proposed where the shared workspace is divided
                            spatially to dynamic virtual zones, each having separate safety features.
                            The zones are modeled and monitored by a single depth sensor overseeing the shared workspace.
                            For interaction and feedback, a projector-camera user-interface is implemented.
                            The proposed and a baseline safety system are experimentally evaluated in a simple assembly task.
                        </p>
                    </td>
                </tr>
                </tbody>
                </table>

                <!-- Paper 2-->
                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/robots.png" alt="blind-date" width="190">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://ieeexplore.ieee.org/abstract/document/7989091">
                            <papertitle>Robustifying Correspondence Based 6D Object Pose Estimation</papertitle>
                        </a>
                        <br>
                        <strong>Antti Hietanen</strong>, Jussi Halme, Anders Glent Buch, Jyrki Latokartano and Joni-Kristian Kämäräinen
                        <br>
                        <em>International Conference on Robotics and Automation (ICRA)</em>, 2017
                        <br>
                        <a href="https://ieeexplore.ieee.org/abstract/document/7989091">paper</a> |
                        <a href="https://www.youtube.com/watch?v=U1GnHAlLaPE">video</a>
                        <p>
                            <!--
                            In computer vision, 3D-to-3D correspondences can be used to fully localize the target object from
                            the sensor input, i.e. estimate the object 6D pose.
                            However, repetitive or simple object geometry can significantly decrease the estimation accuracy and therefore in this work two robustifying
                            methods were proposed: curvature filtering and region pruning.
                            The former method removes points from the object surface that are within low curvature areas.
                            The latter processes the surface as local regions for which a good combination is sought by a trial-and-error procedure.
                            Based on the experiments, the relatively simple algorithms were able to improve the accuracy of several pose estimation methods and were later utilized in a vision guided maintenance task where a tool of an autonomous ground vehicle was changed.
                            -->
                            We propose two methods to robustify 3D-to-3D correspondence-based object pose estimation: curvature filtering and region pruning.
                            The former method removes points from the object surface that are within low curvature areas and the latter processes the surface as local regions for which a good combination is sought by a trial-and-error procedure.
                            Based on the experiments, the relatively simple algorithms are able to improve the accuracy of several pose estimation methods and were later utilized in a vision guided maintenance task where a tool of an autonomous ground vehicle was changed.
                        </p>
                    </td>
                </tr>
                </tbody>
                </table>




                <!-- Paper 1 -->
                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/chairs.png" alt="blind-date" width="190">
                    </td>
                    <td width="75%" valign="middle">
                        <a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231215018433">
                            <papertitle>A Comparison of Feature Detectors and Descriptors for Object Class Matching</papertitle>
                        </a>
                        <br>
                        <strong>Antti Hietanen</strong>, Jukka Lankinen, Joni-Kristian Kämäräinen, Anders Glent Buch, Norbert Krüger
                        <br>
                        <em>Neurocomputing</em>, 2016
                        <br>
                        <a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231215018433">paper</a>
                        <p>
                            The work extends the well known 2D local feature detector and descriptor
                            benchmark to class matching settings.
                            In particular, we were interested to study how well the recent feature detectors and descriptors
                            can find “common codes” between two object examples from the same class.
                            <!--
                            For instance, a scooter and Harley-Davidson are both from the motorcycle class but there
                            is a clear difference between the two in terms of shape and appearance.
                            In contrast, one can still recognize semantically similar parts from the objects such a handlebar or
                            pair of wheels.
                            -->
                            As the main results from the experiments, the performance of detector-descriptor pairs on class matching settings is poor and specialized descriptors for visual class parts and regions are needed.
                        </p>
                    </td>
                </tr>
                </tbody>
                </table>

                <!-- Media -->
                <heading>In Media</heading>
                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                <tr>
                    <p>
                        <a href="https://www.tuni.fi/en/news/antti-hietanen-safe-and-autonomous-robots-using-computer-vision"><strong>Antti Hietanen: Safe and autonomous robots using computer vision</strong></a>
                    </p>
                    <p>
                        <a href="https://www.apu.fi/artikkelit/nahdaan-huomenna"><strong>Tampereella on unelma. Siellä halutaan kehittää koneille yhtä hyvä näköjärjestelmä kuin ihmisillä on. Tai mielellään parempi. Sitten meidät voidaan vapauttaa töistä.</strong></a>
                    </p>
                    <p>
                        <a href="https://sensible4.fi/2021/02/09/self-driving-test-in-finnish-lapland-how-did-sensible-4-software-perform-in-harsh-winter-conditions/"><strong>Self-driving test in Finnish Lapland – How did Sensible 4 software perform in harsh winter conditions?</strong></a>
                    </p>
                    <p>
                        <a href="https://news.cision.com/fi/house-of-lapland/r/sensible-4-n-itsestaan-ajavan-ajoneuvon-ohjelmisto-testataan-lapissa,c3257111"><strong>Sensible 4:n itsestään ajavan ajoneuvon ohjelmisto testataan Lapissa</strong></a>
                    </p>
                    <p>
                        <a href="https://sensible4.fi/2021/07/21/imu-and-autonomous-vehicles/"><strong>Inertial Measurement Units for localization and navigation of autonomous vehicles</strong></a>
                    </p>
                </tr>
                </tbody>
                </table>
            </td>
        </tr>
    </table>

    <!-- Link to the original template -->
    </table>
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
            <td>
                <br>
                <p align="right">
                    <font size="2">
                        Template <a href="https://github.com/jonbarron/website"><strong>link</strong></a>.
                    </font>
                </p>
            </td>
        </tr>
    </table>


    </body>
</html>